{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q6zBs096pHhT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import t, f, norm\n",
        "np.set_printoptions(precision= 5, suppress= True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lu8wI3zxpHhV"
      },
      "outputs": [],
      "source": [
        "class SLR():\n",
        "    \"\"\"\n",
        "    Author: Lucius Vo <https://github.com/vohuynhquangnguyen>\n",
        "    Construct a simple linear regression (SLR) model and conduct required estimations and hypothesis test.\n",
        "    Methods of estimating the model's parameters and hypothesis test are based on Mendenhall and Sincich (2013, p.96-165).\n",
        "\n",
        "    References:\n",
        "    1. Mendenhall, William, Sincich, Terry T. A Second Course in Statistics - Regression Analysis (7th Edition). Pearson, 2013\n",
        "    2. Montgomery, Douglas C., Runger, George C. Applied Statistics and Probability for Engineer (7th Edition). Wiley, 2018\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        super(SLR, self).__init__\n",
        "        pass\n",
        "\n",
        "    def load_data(self, src: str, X_header: str, Y_header: str):\n",
        "        \"\"\"\n",
        "        Load the dataset from a .csv file using Pandas file handler.\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(src)\n",
        "        self.X_header = X_header; self.Y_header = Y_header\n",
        "        self.X = np.array(df[X_header])\n",
        "        self.Y = np.array(df[Y_header])\n",
        "        pass\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Compute the parameters of the fitted model using the ordinary least square (OLS) method.\n",
        "        \"\"\"\n",
        "        self.Xbar = np.mean(self.X)\n",
        "        self.Ybar = np.mean(self.Y)\n",
        "\n",
        "        self.SS_xx = sum((np.power(x - self.Xbar, 2) for x in self.X))\n",
        "        self.SS_yy = sum((np.power(y - self.Ybar, 2) for y in self.Y))\n",
        "        self.SS_xy = sum(((x - self.Xbar) * (y - self.Ybar) for (x,y) in zip(self.X,self.Y)))\n",
        "\n",
        "        self.B1hat = self.SS_xy / self.SS_xx\n",
        "        self.B0hat = self.Ybar - self.B1hat * self.Xbar\n",
        "        pass\n",
        "\n",
        "    def estimate_variance_residuals(self):\n",
        "        \"\"\"\n",
        "        Estimate the variance of residuals.\n",
        "        \"\"\"\n",
        "        self.e = np.array([y - (self.B0hat + self.B1hat * x) for (x,y) in zip(self.X, self.Y)])\n",
        "        self.SS_E = sum((np.power(y - (self.B0hat + self.B1hat * x), 2) for (x,y) in zip(self.X, self.Y)))\n",
        "        self.s2 = self.SS_E / (len(self.Y) - 2)\n",
        "        pass\n",
        "\n",
        "    def ANOVA(self):\n",
        "        \"\"\"\n",
        "        Conduct the analysis of variance (ANOVA) on the fitted model.\n",
        "        \"\"\"\n",
        "        self.SS_R = sum((np.power((self.B0hat + self.B1hat * x) - self.Ybar, 2) for x in self.X))\n",
        "        self.SS_T = self.SS_R + self.SS_E\n",
        "        self.Rsquare = self.SS_R / self.SS_T\n",
        "        pass\n",
        "\n",
        "    def conduct_statistical_inference(self, level: float = 0.95):\n",
        "        \"\"\"\n",
        "        Conduct the hypothesis tests on the fitted model at the significant level (default is 95%).\n",
        "        \"\"\"\n",
        "        self.level = level\n",
        "        self.estimate_variance_residuals()\n",
        "        self.ANOVA()\n",
        "\n",
        "        # Test on the estimated slope and intercept (two-tailed T-test):\n",
        "        t_c = t.ppf((1 - level)/2, df = len(self.Y) - 2)\n",
        "        self.SE_B1hat = np.power(self.s2 / self.SS_xx, 1/2)\n",
        "        self.T_B1hat = (self.B1hat - 0) / self.SE_B1hat\n",
        "        self.SE_B0hat = np.power(self.s2 * ( 1 / len(self.Y) + self.Xbar ** 2 / self.SS_xx), 1/2)\n",
        "        self.T_B0hat = (self.B0hat - 0) / self.SE_B0hat\n",
        "\n",
        "        self.pval_B1 = t.sf(abs(self.T_B1hat), len(self.Y) - 2) * 2\n",
        "        self.pval_B0 = t.sf(abs(self.T_B0hat), len(self.Y) - 2) * 2\n",
        "\n",
        "        # Test on the Pearson correlation coefficient (two-tailed T-test):\n",
        "        self.r = np.power(self.SS_xy / (self.SS_xx * self.SS_yy), 1/2)\n",
        "        self.t_r = np.power(self.r * ((len(self.Y) - 2) / (1 - self.r ** 2)), 1/2)\n",
        "        self.pval_r = t.sf(abs(self.t_r), df = len(self.Y) - 2) * 2\n",
        "\n",
        "        # Test on the significant of regression (two-tailed T-test):\n",
        "        self.F = (self.SS_R / 1) / (self.SS_E / (len(self.Y) - 2))\n",
        "        self.pval_F = f.sf(self.F, 1, len(self.Y) - 2)\n",
        "        pass\n",
        "\n",
        "    def compute_intervals(self, level: float = 0.95):\n",
        "        \"\"\"\n",
        "        Compute the confidence intervals and the prediction intervals:\n",
        "        \"\"\"\n",
        "        t_c = t.ppf((1 - level)/2, df = len(self.Y) - 2)\n",
        "\n",
        "        # Confidence interval for the parameters:\n",
        "        self.B0hat_CI_lwr = self.B0hat - t_c * self.SE_B0hat\n",
        "        self.B0hat_CI_upr = self.B0hat + t_c * self.SE_B0hat\n",
        "\n",
        "        self.B1hat_CI_lwr = self.B1hat - t_c * self.SE_B1hat\n",
        "        self.B1hat_CI_upr = self.B1hat + t_c * self.SE_B1hat\n",
        "\n",
        "        # Confidence interval of mean response at x:\n",
        "        self.func = lambda x: self.B0hat + self.B1hat * x\n",
        "        self.Yhat_CI_lwr = lambda x: \\\n",
        "          self.B0hat + self.B1hat * x - t_c * np.sqrt(self.s2 * (1/len(self.Y) + np.power(x - self.Xbar,2) / (self.SS_xx)))\n",
        "        self.Yhat_CI_upr = lambda x: \\\n",
        "          self.B0hat + self.B1hat * x + t_c * np.sqrt(self.s2 * (1/len(self.Y) + np.power(x - self.Xbar,2) / (self.SS_xx)))\n",
        "\n",
        "        # Prediction interval of mean response at x:\n",
        "        self.yhat_PI_lwr = lambda x: \\\n",
        "          self.B0hat + self.B1hat * x - t_c * np.sqrt(self.s2 * (1 + 1/len(self.Y) + np.power(x - self.Xbar,2) / (self.SS_xx)))\n",
        "        self.yhat_PI_upr = lambda x: \\\n",
        "          self.B0hat + self.B1hat * x + t_c * np.sqrt(self.s2 * (1 + 1/len(self.Y) + np.power(x - self.Xbar,2) / (self.SS_xx)))\n",
        "        pass\n",
        "\n",
        "    def visualize(self):\n",
        "        \"\"\"\n",
        "        Visualize the entire analysis process:\n",
        "        \"\"\"\n",
        "\n",
        "        # Visualize the distribution of the dataset:\n",
        "        fig, axs = plt.subplots(1, 2, figsize = (8,4), dpi = 100)\n",
        "        nbins = \\\n",
        "         (np.max(self.X) - np.min(self.X)) * np.power(len(self.Y), 1/3) / (3.49 * np.std(self.X, ddof= 1))\n",
        "        sns.histplot(self.X, bins = int(nbins), ax = axs[0])\n",
        "        axs[0].set_title(f\"Histogram of {self.X_header}\")\n",
        "        axs[0].grid(True)\n",
        "\n",
        "        nbins = \\\n",
        "         (np.max(self.Y) - np.min(self.Y)) * np.power(len(self.Y), 1/3) / (3.49 * np.std(self.Y, ddof= 1))\n",
        "        sns.histplot(self.Y, bins = int(nbins), ax = axs[1])\n",
        "        axs[1].set_title(f\"Histogram of {self.Y_header}\")\n",
        "        axs[1].grid(True)\n",
        "        fig.tight_layout()\n",
        "\n",
        "        # Visualize the regression analysis:\n",
        "        fig, axs = plt.subplots(1, 3, figsize = (15,4), dpi = 100)\n",
        "        sns.scatterplot({f\"{self.X_header}\": self.X, f\"{self.Y_header}\": self.Y},\n",
        "                        x =  f\"{self.X_header}\", y = f\"{self.Y_header}\", ax = axs[0])\n",
        "        sns.lineplot({\"x\": self.X, \"y\": self.func(self.X)},\n",
        "                     x = \"x\", y = \"y\", color = 'red', linestyle = 'solid', linewidth = 1.0, ax = axs[0])\n",
        "        sns.lineplot({\"x\": self.X, \"y\": self.Yhat_CI_lwr(self.X)},\n",
        "                     x = \"x\", y = \"y\", color = 'red', linestyle = 'dashdot', linewidth = 0.75, ax = axs[0])\n",
        "        sns.lineplot({\"x\": self.X, \"y\": self.Yhat_CI_upr(self.X)},\n",
        "                     x = \"x\", y = \"y\", color = 'red', linestyle = 'dashdot', linewidth = 0.75, ax = axs[0])\n",
        "        sns.lineplot({\"x\": self.X, \"y\": self.yhat_PI_lwr(self.X)},\n",
        "                     x = \"x\", y = \"y\", color = 'red', linestyle = 'dotted', linewidth = 0.55, ax = axs[0])\n",
        "        sns.lineplot({\"x\": self.X, \"y\": self.yhat_PI_upr(self.X)},\n",
        "                     x = \"x\", y = \"y\", color = 'red', linestyle = 'dotted', linewidth = 0.55, ax = axs[0])\n",
        "        axs[0].set_title(f\"{self.Y_header} vs. {self.X_header}\")\n",
        "        axs[0].grid(True)\n",
        "\n",
        "        # Visualize the residual analysis:\n",
        "        sns.scatterplot({f\"{self.Y_header}\": self.Y, \"Residual\": self.e},\n",
        "                        x = f\"{self.Y_header}\", y = \"Residual\",\n",
        "                        ax = axs[1])\n",
        "        axs[1].axhline(y = 0, color = 'red', linestyle = 'dashed', linewidth = 1.0)\n",
        "        axs[1].set_title(f\"Residual Plot\")\n",
        "        axs[1].grid(True)\n",
        "\n",
        "        rank = np.array([(i - 0.375) / (len(self.Y) + 0.25) for i in range(1, len(self.Y) + 1)])\n",
        "        z_e = np.array([norm.ppf(i) for i in rank])\n",
        "        sns.scatterplot({\"Residual\": sorted(self.e), \"Z-score\": z_e},\n",
        "                        x = \"Residual\", y = \"Z-score\", ax = axs[2])\n",
        "        axs[2].set_title(f\"Residual Normality Plot\")\n",
        "        axs[2].grid(True)\n",
        "        fig.tight_layout()\n",
        "        pass\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"\n",
        "        Generate a regression analysis report.\n",
        "        \"\"\"\n",
        "        print(f\"\\nModel: {self.Y_header} ~ {self.X_header}\")\n",
        "        print(f\"\\n\\t  Min \\t Q1 \\t Median \\t Q3 \\tMax\")\n",
        "        print(f\"Residuals: {np.min(self.e):.5f} \\t {np.quantile(self.e, 0.25):.5f} \\t {np.median(self.e):.5f} \\t {np.quantile(self.e, 0.75):.5f} \\t {np.max(self.e):.5f}\")\n",
        "\n",
        "        print(f\"\\n\\t  Coefficient \\t Std. Error \\t Lower-bound CI \\t Upper-bound CI \\t t-Statistic \\t Pr(>|t|) at {100 * self.level}%\")\n",
        "        print(f\"Intercept: {self.B0hat:.5f} \\t {self.SE_B0hat:.5f} \\t {self.B0hat_CI_lwr:.5f} \\t {self.B0hat_CI_upr:.5f} \\t {self.T_B0hat:.5f} \\t {self.pval_B0:.5f}\")\n",
        "        print(f\"{self.X_header}: {self.B1hat:.5f} \\t {self.SE_B1hat:.5f} \\t {self.B1hat_CI_lwr:.5f} \\t {self.B1hat_CI_upr:.5f} \\t {self.T_B1hat:.5f} \\t {self.pval_B1:.5f}\")\n",
        "\n",
        "        print(f\"\\nResidual Std. Error: {np.power(self.s2,1/2):.5f} on {len(self.Y) - 2} DF\")\n",
        "        print(f\"R-square: {self.Rsquare:.5f}\")\n",
        "        print(f\"Pearson correlation coef.: {self.r:.5f}, p-value: {self.pval_r:.5f}\")\n",
        "        print(f\"F-statistic: {self.F:.5f} on 1 predictor and {len(self.Y) - 2} DF, p-value: {self.pval_F:.5f}\")\n",
        "        pass\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        self.fit()\n",
        "        self.estimate_variance_residuals()\n",
        "        self.conduct_statistical_inference()\n",
        "        self.compute_intervals()\n",
        "        self.generate_report()\n",
        "        self.visualize()\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "g95BKXbqpHhX",
        "outputId": "a9f723d4-1f8b-4c31-adea-cfb4f8c5280b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model: Sale_Price ~ Market_Val\n",
            "\n",
            "\t  Min \t Q1 \t Median \t Q3 \tMax\n",
            "Residuals: -282.17064 \t -24.82893 \t 1.80737 \t 29.79117 \t 188.79201\n",
            "\n",
            "\t  Coefficient \t Std. Error \t Lower-bound CI \t Upper-bound CI \t t-Statistic \t Pr(>|t|) at 95.0%\n",
            "Intercept: 1.35868 \t 13.76817 \t 28.79237 \t -26.07501 \t 0.09868 \t 0.92166\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Unknown format code 'f' for object of type 'str'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m slr \u001b[38;5;241m=\u001b[39m SLR()\n\u001b[0;32m      2\u001b[0m slr\u001b[38;5;241m.\u001b[39mload_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./dataset/TAMPALMS.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarket_Val\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSale_Price\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m slr\u001b[38;5;241m.\u001b[39mrun()\n",
            "Cell \u001b[1;32mIn[8], line 192\u001b[0m, in \u001b[0;36mSLR.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconduct_statistical_inference()\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_intervals()\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_report()\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisualize()\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[8], line 177\u001b[0m, in \u001b[0;36mSLR.generate_report\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m  Coefficient \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Std. Error \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Lower-bound CI \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Upper-bound CI \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m t-Statistic \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Pr(>|t|) at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntercept: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB0hat\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSE_B0hat\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB0hat_CI_lwr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB0hat_CI_upr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT_B0hat\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpval_B0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_header\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB1hat\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSE_B1hat\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB1hat_CI_lwr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB1hat_CI_upr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT_B1hat\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpval_B1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResidual Std. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms2,\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m DF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR-square: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRsquare\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: Unknown format code 'f' for object of type 'str'"
          ]
        }
      ],
      "source": [
        "slr = SLR()\n",
        "slr.load_data(\"./dataset/TAMPALMS.csv\", \"Market_Val\", \"Sale_Price\")\n",
        "slr.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
