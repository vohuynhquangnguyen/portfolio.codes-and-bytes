{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Linear Regression with Gradient Descent From Scratch\n",
    "In this notebook, we will (1) learn how to derive and build a gradient descent algorithm for linear regression, (2) compare the results yield by the least square method with those by the gradient descent algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset used in this notebook is extracted from the textbook namely **Applied Statistics and Probability for Engineers (7th Edition)** by Montgomery et al.; the dataset is in Chapter 1, p.13 and p.14. The data in this dataset were collected in an observational study in a semiconductor manufacturing plant. The variables reported in this dataset are **pull strength**, **wire length**, and **die height**. The objective of this study is to predict the pull strength, which is closely related to the bond strength, based on the length of the wire and the height of the die. By investigating the bond strength, engineers can evaluate bond failure modalities or determine compliance with specified bond strength requirements, for instance the MIL-STD-810G standard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##\n",
    "# Load the dataset:\n",
    "#\n",
    "dataframe = pd.read_csv(\"../linear-regression/dataset/wirepullstrength.csv\")\n",
    "X1 = dataframe[\"Wire Length\"]\n",
    "X2 = dataframe[\"Die Height\"]\n",
    "Y = dataframe[\"Pull Strength\"]\n",
    "\n",
    "##\n",
    "# Visualize the dataset:\n",
    "#\n",
    "fig, axs = plt.subplots(1,2, figsize = (10,5), dpi = 100)\n",
    "axs[0].scatter(X1, Y)\n",
    "axs[0].set_title('Wire Length vs. Pull Strength')\n",
    "axs[0].set_xlabel('Wire Length')\n",
    "axs[0].set_ylabel('Pull Strength')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].scatter(X2, Y)\n",
    "axs[1].set_title('Die Height vs. Pull Strength')\n",
    "axs[1].set_xlabel('Die Height')\n",
    "axs[1].set_ylabel('Pull Strength')\n",
    "axs[1].grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Square Method\n",
    "### Backgrounds\n",
    "Consider a simple dataset of $n$ observations ($\\mathbf{x} = \\{x_i\\}, \\mathbf{y} = \\{y_i\\} \\text{  } \\forall i = \\{1,2,3,...,n\\}$) where $\\mathbf{x}$ is an independent variable, and $\\mathbf{y}$ is a dependent variable. To describe the linear relationship between $\\mathbf{x}$ and $\\mathbf{y}$, we use the following model $\\mathbf{y} = \\beta_0 + \\beta_1\\mathbf{x}$, where $\\beta_0$ and $\\beta_1$ are the parameters of the linear model.\n",
    "\n",
    "Our goal is to find the values of $\\beta_0$ and $\\beta_1$ such that the model fits the data the best. In other words, we want to find the values of $\\beta_0$ and $\\beta_1$ such that the difference between the observed values of the dependent variable and the values predicted by the model, also know as the residual, is the smallest:\n",
    "$$\n",
    "\\text{min} \\quad |(\\beta_0 + \\beta_1\\mathbf{x}) - \\mathbf{y}|\n",
    "$$\n",
    "\n",
    "Let $F(\\beta_0, \\beta_1) = |(\\beta_0 + \\beta_1\\mathbf{x}) - \\mathbf{y}|$ be the function that describes the difference between the observed values of the dependent variable and the values predicted by the model. Clearly $F(\\beta_0, \\beta_1)$ is a piecewise function, which is surprising difficult to find the extrema. Hence, we usually transform the function $F$ to a function denoted as $J$ such that:\n",
    "$$\n",
    "J(\\beta_0, \\beta_1) = F^2 = \\Big( (\\beta_0 + \\beta_1\\mathbf{x}) - \\mathbf{y} \\Big)^2\n",
    "$$\n",
    "\n",
    "As a result, we can easily find the values $\\beta_0$ and $\\beta_1$ that yields the smallest residual by finding the extrema of $J(\\beta_0, \\beta_1)$ via partial derivative:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J(\\beta_0, \\beta_1)}{\\partial \\beta_0} &= 2\\Big( (\\beta_0 + \\beta_1\\mathbf{x}) - \\mathbf{y} \\Big) \\\\\n",
    "\\frac{\\partial J(\\beta_0, \\beta_1)}{\\partial \\beta_1} &= 2\\Big( (\\beta_0 + \\beta_1\\mathbf{x}) - \\mathbf{y} \\Big)\\mathbf{x} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Formulas\n",
    "For n-dimensional data ($\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{m\\times n}$):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{Y} &= \\beta_0 + \\beta_1\\mathbf{X} \\\\\n",
    "\\beta_1 &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n",
    "\\beta_0 &= E[\\mathbf{Y}] - \\beta_1\\mathbf{X}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For 1-dimensional data ($\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{m}$):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{y} &= \\beta_0 + \\beta_1\\mathbf{x} \\\\\n",
    "\\beta_1 &= \\frac{\\sum_{i = 1}^m (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i = 1}^m (x_i - \\bar{x})^2} \\\\\n",
    "\\beta_0 &= \\bar{y} - \\beta_1\\bar{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Below is how the least square method is applied to find parameters of the model for our aforemetioned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Simple Linear Regression Using Least Square Method (1-dimensional):\n",
    "#\n",
    "def one_dimensional_least_square(X, Y):\n",
    "    \"\"\"\n",
    "    Compute parameters of the least square linear model.\n",
    "    \"\"\"\n",
    "    numerator = np.dot([(x - np.mean(X)) for x in X], [(y - np.mean(Y)) for y in Y])\n",
    "    denominator = np.sum([(x - np.mean(X)) ** 2 for x in X])\n",
    "    beta_1 = numerator / denominator\n",
    "    beta_0 = np.mean(Y) - beta_1 * np.mean(X)\n",
    "    return beta_0, beta_1\n",
    "\n",
    "beta_0_X1, beta_1_X1 = one_dimensional_least_square(X1, Y)\n",
    "Yhat_1 = beta_0_X1 + beta_1_X1 * X1\n",
    "f1 = lambda x: beta_0_X1 + beta_1_X1 * x\n",
    "\n",
    "beta_0_X2, beta_1_X2 = one_dimensional_least_square(X2, Y)\n",
    "Yhat_2 = beta_0_X2 + beta_1_X2 * X2\n",
    "f2 = lambda x: beta_0_X2 + beta_1_X2 * x\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize = (15,6), dpi = 100)\n",
    "axs[0].scatter(X1, Y)\n",
    "axs[0].plot(X1, f1(X1), color = 'red')\n",
    "axs[0].set_title('Wire Length vs. Pull Strength')\n",
    "axs[0].set_xlabel('Wire Length')\n",
    "axs[0].set_ylabel('Pull Strength')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].scatter(X2, Y)\n",
    "axs[1].plot(X2, f2(X2), color = 'red')\n",
    "axs[1].set_title('Die Height vs. Pull Strength')\n",
    "axs[1].set_xlabel('Die Height')\n",
    "axs[1].set_ylabel('Pull Strength')\n",
    "axs[1].grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Method\n",
    "### Backgrounds\n",
    "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. In this scenario, the function that we are interested in is the mean of squared residuals:\n",
    "$$\n",
    "\\frac{1}{m}\\sum_{i = 1}^m(\\beta_0 + \\beta_1x_i - y_i)^2\n",
    "$$\n",
    "\n",
    "In practice, this function will be scaled by a factor of $\\frac{1}{2}$ since it makes the differentiation task easier:\n",
    "$$\n",
    "\\text{min} \\quad J(\\beta_0, \\beta_1) = \\frac{1}{2m}\\sum_{i = 1}^m(\\beta_0 + \\beta_1x_i - y_i)^2\n",
    "$$\n",
    "with $J(\\beta_0, \\beta_1)$ is the cost function, a.k.a. the function that we want to find the local mininum. \n",
    "\n",
    "The gradient descent algorithm has the following steps:\n",
    "* Step 1: initialize $\\beta_0$ and $\\beta_1$. The intial values of the parameters $\\beta_1$ and $\\beta_0$ are usually drawn from a truncated normal distribution with zero mean and a specific standard deviation $\\mathcal{N}(0, \\sigma = \\sqrt{\\frac{2}{m}})$\n",
    "* Step 2: compute the gradient of the loss function in respect to $\\beta_0$ and $\\beta_1$. In this scenario, the loss function is the mean of squared residuals:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\beta_1} &= \\frac{1}{m}\\sum_{i=1}^m(\\beta_0 + \\beta_1x_i - y_i)(x_i) \\\\\n",
    "\\nabla_{\\beta_0} &= \\frac{1}{m}\\sum_{i=1}^m(\\beta_0 + \\beta_1x_i - y_i) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "* Step 3: update $\\beta_0$ and $\\beta_1$ iteratively. The parameters at the $k + 1$ iteration equal to the parameters at the $k$ iteration minus the learning step, which is the product of the learning rate $\\eta$ and the gradients.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_1^{k+1} &= \\beta_1^{k} - \\eta\\nabla_{\\beta_1}  \\\\\n",
    "\\beta_0^{k+1} &= \\beta_0^{k} - \\eta\\nabla_{\\beta_0}  \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Below is how the gradient descent method is used to find parameters of the linear model for our aforementioned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Simple linear regression using gradient descent:\n",
    "#\n",
    "def initializer(X, Y, type = 'he_norm'):\n",
    "  \"\"\"\n",
    "  Initializes the weights of a linear regression model.\n",
    "  \"\"\"\n",
    "  if type == 'zero':\n",
    "    weight = np.zeros(Y.shape[0])\n",
    "  elif type == 'he_norm':\n",
    "    weight = np.random.normal(loc = 0, scale = np.sqrt(2 / len(Y)))\n",
    "  else:\n",
    "    raise ValueError('Invalid initialization type: {}'.format(type))\n",
    "\n",
    "  return weight\n",
    "\n",
    "def compute_gradients(X, Y, beta_0, beta_1):\n",
    "  \"\"\"\n",
    "  Computes the gradients of the loss function with respect to the weights of a\n",
    "  linear regression model.\n",
    "  \"\"\"\n",
    "  L = 1 / (2 * len(Y)) * ((beta_0 + beta_1 * X) - Y)**2\n",
    "  dLdb1 = np.clip(1 / len(Y) * np.dot((beta_0 + beta_1 * X) - Y, X), a_max = 100, a_min = -100)\n",
    "  dLdb0 = np.clip(1 / len(Y) * np.sum((beta_0 + beta_1 * X) - Y), a_max = 100, a_min = -100)\n",
    "  \n",
    "  return dLdb0, dLdb1, L\n",
    "\n",
    "def gradient_descent(X, Y, lr, num_iter):\n",
    "  \"\"\"\n",
    "  Trains a linear regression model using gradient descent.\n",
    "  \"\"\"\n",
    "\n",
    "  beta_0 = initializer(X, Y, type = 'he_norm')\n",
    "  beta_1 = initializer(X, Y, type = 'he_norm')\n",
    "\n",
    "  for i in range(num_iter + 1):\n",
    "    dLdb0, dLdb1, L = compute_gradients(X, Y, beta_0, beta_1)\n",
    "    beta_0 -= lr * dLdb0\n",
    "    beta_1 -= lr * dLdb1\n",
    "\n",
    "  return beta_0, beta_1\n",
    "\n",
    "beta_0_X1, beta_1_X1 = gradient_descent(X1, Y, lr = 0.01, num_iter = 1000)\n",
    "f1 = lambda x: beta_0_X1 + beta_1_X1 * x\n",
    "\n",
    "beta_0_X2, beta_1_X2 = gradient_descent(X2, Y, lr = 0.00001, num_iter = 1000)\n",
    "f2 = lambda x: beta_0_X2 + beta_1_X2 * x\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize = (15,6), dpi = 100)\n",
    "axs[0].scatter(X1, Y)\n",
    "axs[0].plot(X1, f1(X1), color = 'red')\n",
    "axs[0].set_title('Wire Length vs. Pull Strength')\n",
    "axs[0].set_xlabel('Wire Length')\n",
    "axs[0].set_ylabel('Pull Strength')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].scatter(X2, Y)\n",
    "axs[1].plot(X2, f2(X2), color = 'red')\n",
    "axs[1].set_title('Die Height vs. Pull Strength')\n",
    "axs[1].set_xlabel('Die Height')\n",
    "axs[1].set_ylabel('Pull Strength')\n",
    "axs[1].grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
