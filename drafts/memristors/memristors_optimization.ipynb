{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x2884c3c6910>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.printoptions(suppress= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalKaczmarz(A, b, l = 1.0, MAX_TOL= 1e-16, MAX_ITER= 1000):\n",
    "    \"\"\"\n",
    "    Implement the Kaczmarz algorithm to solve a system of linear equation.\n",
    "\n",
    "    The algorithm works under the principle of finding an estimated solution (xhat) that satisfies \n",
    "    ||A @ xhat - b||^2 < e, with e is the tolerance value.\n",
    "    \"\"\"\n",
    "    assert (0 <= l <= 1), \"The relaxation parameter must be of the range 0 < l < 1!\"\n",
    "    \n",
    "    X = np.random.normal(size= (A.shape[1], 1))\n",
    "    b = np.reshape(b, (A.shape[0], 1))\n",
    "\n",
    "    iter = 0\n",
    "    residual = 1e+6\n",
    "    while (residual > MAX_TOL) and (iter < MAX_ITER):\n",
    "        idx = iter % A.shape[0]\n",
    "        a_i = A[idx, :]\n",
    "        b_i = b[idx]\n",
    "\n",
    "        change = np.reshape(l * np.multiply(b_i - np.dot(a_i, X), a_i) / np.power(np.linalg.norm(a_i), 2),\n",
    "                            (A.shape[1],1))\n",
    "        Xnew = X + change\n",
    "        residual = np.linalg.norm(b - np.dot(A, Xnew))\n",
    "        X = np.copy(Xnew)\n",
    "        iter += 1\n",
    "\n",
    "    return X\n",
    "\n",
    "def randomizedKaczmarz(A, b, l = 1.0, MAX_TOL= 1e-16, MAX_ITER= 1000):\n",
    "    \"\"\"\n",
    "    Implement the Kaczmarz algorithm to solve a system of linear equation.\n",
    "\n",
    "    The algorithm works under the principle of finding an estimated solution (xhat) that satisfies \n",
    "    ||A @ xhat - b||^2 < e, with e is the tolerance value.\n",
    "    \"\"\"\n",
    "    assert (0 <= l <= 1), \"The relaxation parameter must be of the range 0 < l < 1!\"\n",
    "\n",
    "    X = np.random.normal(size= (A.shape[1], 1))\n",
    "    b = np.reshape(b, (A.shape[0], 1))\n",
    "    probs = np.power(np.linalg.norm(A, axis = 1) / np.linalg.norm(A), 2)\n",
    "\n",
    "    iter = 0\n",
    "    residual = 1e+6\n",
    "    while (residual > MAX_TOL) and (iter < MAX_ITER):\n",
    "        idx = np.random.choice(A.shape[0], p = probs)\n",
    "        a_i = A[idx, :]\n",
    "        b_i = b[idx]\n",
    "\n",
    "        change = np.reshape(l * np.multiply(b_i - np.dot(a_i, X), a_i) / np.power(np.linalg.norm(a_i), 2),\n",
    "                            (A.shape[1],1))\n",
    "        Xnew = X + change\n",
    "        residual = np.linalg.norm(b - np.dot(A, Xnew))\n",
    "        X = np.copy(Xnew)\n",
    "        iter += 1\n",
    "\n",
    "    return X\n",
    "\n",
    "def admm(d, A, b, rho= 1, max_iter= 1000):\n",
    "    \"\"\"\n",
    "    Implement the alternating direction method of multipliers (ADMM) optimization algorithm to solve a linear program.\n",
    "    The implementation is based on the formulas mentioned in Liu et al. (2018).\n",
    "\n",
    "    The linear program has the following form: min d^Tx, s.t. Ax = b\n",
    "    \"\"\"\n",
    "    d = d.T\n",
    "    b = b.T\n",
    "\n",
    "    # Extract the number of variables and constraints from the matrix dimensions\n",
    "    no_of_vars = A.shape[1]\n",
    "    no_of_constraints = A.shape[0]\n",
    "\n",
    "    # Initialize the dual variables y and mu\n",
    "    y  = np.random.normal(size= (1,no_of_vars))\n",
    "    mu = np.random.normal(size= (1,no_of_vars))\n",
    "\n",
    "    # Construct the augmented Lagrangian matrix\n",
    "    C = np.block([\n",
    "        [rho * np.eye(no_of_vars), A.T],\n",
    "        [A, np.zeros((no_of_constraints,no_of_constraints))]\n",
    "                ])\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Solve for the primal variable x\n",
    "        alpha = y - (1/rho) * (mu + d)\n",
    "        m = np.block([rho * alpha, b]).T\n",
    "        x = normalKaczmarz(C, m)[: no_of_vars]\n",
    "\n",
    "        # Apply soft-thresholding to update the dual variable y\n",
    "        beta = x + (1/rho) * mu.T\n",
    "        beta[beta < 0] = 0\n",
    "        y = beta.T\n",
    "\n",
    "        # Update the dual variable mu\n",
    "        mu = mu + rho * (x.T - y)\n",
    "\n",
    "    return (x,y,mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.05017231e-05]\n",
      " [ 1.57894048e+00]\n",
      " [ 1.68424878e+00]\n",
      " [-3.76536906e-05]\n",
      " [ 1.32870161e-05]\n",
      " [ 3.63144096e+00]]\n"
     ]
    }
   ],
   "source": [
    "d = np.array([-1, -2, -4, 0, 0, 0])\n",
    "A = np.array([\n",
    "    [3, 1, 5, 1, 0, 0],\n",
    "    [1, 4, 1, 0, 1, 0],\n",
    "    [2, 0, 2, 0, 0, 1]])\n",
    "b = np.array([10, 8, 7])\n",
    "\n",
    "x, _ , _ = admm(d, A, b)\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
